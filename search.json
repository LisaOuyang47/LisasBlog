[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Data-Analysis-of-Amazon-Sales-Dataset.html",
    "href": "posts/Data-Analysis-of-Amazon-Sales-Dataset.html",
    "title": "Sentiment Analysis in NLP of Amazon Sales Data Project",
    "section": "",
    "text": "This Amazon sales dataset contains review and rating information for over a thousand products. In the dataset, product IDs, names, categories, product descriptions, links to product photos and links to their official websites are listed; various prices (before discount, after discount and discount percentage) are also provided; finally, users‚Äô ratings and reviews of the products they purchased, including their personal information (IDs and names) are also available in this dataset. This project will mainly focus on further analysis of the content and ratings of the customer‚Äôs reviews. Although this project is mainly conducted with Natural Language Processing (NLP), it will be more centered on one of the NLP techniques: sentiment analysis.\n\n\n\nBased on the data set and my interests, I decided to focus my interest in solving the following problems:\n\nPrice & Rating\n\na. What is the relationship between product price and user rating? b. Does discounting boost customers‚Äô consumption as well as ratings?\n\nNatural Language Processing (NLP)\n\na. Which words are most frequently used by users in review titles and content?\n\nSentiment Analysis\n\na. What is the overall sentiment of users about the product in the dataset? Which words are used most often in negative/positive reviews?\nb. Is there a positive relationship between emotions and ratings? If not, why?"
  },
  {
    "objectID": "posts/Data-Analysis-of-Amazon-Sales-Dataset.html#data-overview",
    "href": "posts/Data-Analysis-of-Amazon-Sales-Dataset.html#data-overview",
    "title": "Sentiment Analysis in NLP of Amazon Sales Data Project",
    "section": "Data Overview",
    "text": "Data Overview\nBased on a simple inspection of the dataset, I found that this dataset has a total of 1465 product records with 16 columns. All of these columns are marked as object, which is incorrect for some of these columns. In addition, I found that there are missing data in the ‚Äúrating‚Äù column of the dataset. And there are unknown characters in the ‚Äúdiscounted_price‚Äù and ‚Äúactual_price‚Äù columns. I will then focus on these issues in the following data cleaning and preprocessing.\n\n1. Remove missing data\n\ndf[df['rating_count'].isnull()]\n\n\n\n\n\n  \n    \n      \n      product_id\n      product_name\n      category\n      discounted_price\n      actual_price\n      discount_percentage\n      rating\n      rating_count\n      about_product\n      user_id\n      user_name\n      review_id\n      review_title\n      review_content\n      img_link\n      product_link\n    \n  \n  \n    \n      282\n      B0B94JPY2N\n      Amazon Brand - Solimo 65W Fast Charging Braide...\n      Computers&Accessories|Accessories&Peripherals|...\n      ‚Çπ199\n      ‚Çπ999\n      80%\n      3.0\n      NaN\n      USB C to C Cable: This cable has type C connec...\n      AE7CFHY23VAJT2FI4NZKKP6GS2UQ\n      Pranav\n      RUB7U91HVZ30\n      The cable works but is not 65W as advertised\n      I have a pd supported car charger and I bought...\n      https://m.media-amazon.com/images/W/WEBP_40237...\n      https://www.amazon.in/Amazon-Brand-Charging-Su...\n    \n    \n      324\n      B0BQRJ3C47\n      REDTECH USB-C to Lightning Cable 3.3FT, [Apple...\n      Computers&Accessories|Accessories&Peripherals|...\n      ‚Çπ249\n      ‚Çπ999\n      75%\n      5.0\n      NaN\n      üíé[The Fastest Charge] - This iPhone USB C cabl...\n      AGJC5O5H5BBXWUV7WRIEIOOR3TVQ\n      Abdul Gafur\n      RQXD5SAMMPC6L\n      Awesome Product\n      Quick delivery.Awesome ProductPacking was good...\n      https://m.media-amazon.com/images/I/31-q0xhaTA...\n      https://www.amazon.in/REDTECH-Lightning-Certif...\n    \n  \n\n\n\n\n\ndf.drop([282,324],inplace = True)\n\n\n\n2. Remove unnecessary chars and convert the data type of ‚Äòprices,‚Äô ‚Äòrating,‚Äô ‚Äòrating_count,‚Äô and ‚Äòdiscount_percentage‚Äô to numeric data type\n\n# replace unnecessary characters\ndf['discounted_price'] = df['discounted_price'].str.replace('‚Çπ', '')\ndf['discounted_price'] = df['discounted_price'].str.replace(',', '')\ndf['actual_price'] = df['actual_price'].str.replace('‚Çπ', '')\ndf['actual_price'] = df['actual_price'].str.replace(',', '')\ndf['rating_count'] = df['rating_count'].str.replace(',', '')\ndf['discount_percentage'] = df['discount_percentage'].str.replace('%', '')\n\n\ndf['discounted_price'] = pd.to_numeric(df['discounted_price'])\ndf['actual_price'] = pd.to_numeric(df['actual_price'])\ndf['discount_percentage'] = pd.to_numeric(df['discount_percentage'])\n\n\n\n3. Drop row(s) that has unvalid value(s)\n\ndf[df['rating'] == '|']\n\n\n\n\n\n  \n    \n      \n      product_id\n      product_name\n      category\n      discounted_price\n      actual_price\n      discount_percentage\n      rating\n      rating_count\n      about_product\n      user_id\n      user_name\n      review_id\n      review_title\n      review_content\n      img_link\n      product_link\n    \n  \n  \n    \n      1279\n      B08L12N5H1\n      Eureka Forbes car Vac 100 Watts Powerful Sucti...\n      Home&Kitchen|Kitchen&HomeAppliances|Vacuum,Cle...\n      2099.0\n      2499.0\n      16\n      |\n      992\n      No Installation is provided for this product|1...\n      AGTDSNT2FKVYEPDPXAA673AIS44A,AER2XFSWNN4LAUCJ5...\n      Divya,Dr Nefario,Deekshith,Preeti,Prasanth R,P...\n      R2KKTKM4M9RDVJ,R1O692MZOBTE79,R2WRSEWL56SOS4,R...\n      Decent product,doesn't pick up sand,Ok ok,Must...\n      Does the job well,doesn't work on sand. though...\n      https://m.media-amazon.com/images/W/WEBP_40237...\n      https://www.amazon.in/Eureka-Forbes-Vacuum-Cle...\n    \n  \n\n\n\n\n\ndf.drop(index = 1279, inplace= True)\n\n\ndf['rating'] = pd.to_numeric(df['rating'])\ndf['rating_count'] = pd.to_numeric(df['rating_count'])\n\n\n\n4. Drop duplicate values\n\ndf.duplicated(keep = False)\n\n0       False\n1       False\n2       False\n3       False\n4       False\n        ...  \n1460    False\n1461    False\n1462    False\n1463    False\n1464    False\nLength: 1462, dtype: bool\n\n\n\n\n5. Summary statistics of the dataset\n\ndf.shape\n\n(1462, 16)\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1462 entries, 0 to 1464\nData columns (total 16 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   product_id           1462 non-null   object \n 1   product_name         1462 non-null   object \n 2   category             1462 non-null   object \n 3   discounted_price     1462 non-null   float64\n 4   actual_price         1462 non-null   float64\n 5   discount_percentage  1462 non-null   int64  \n 6   rating               1462 non-null   float64\n 7   rating_count         1462 non-null   int64  \n 8   about_product        1462 non-null   object \n 9   user_id              1462 non-null   object \n 10  user_name            1462 non-null   object \n 11  review_id            1462 non-null   object \n 12  review_title         1462 non-null   object \n 13  review_content       1462 non-null   object \n 14  img_link             1462 non-null   object \n 15  product_link         1462 non-null   object \ndtypes: float64(3), int64(2), object(11)\nmemory usage: 194.2+ KB\n\n\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      discounted_price\n      actual_price\n      discount_percentage\n      rating\n      rating_count\n    \n  \n  \n    \n      count\n      1462.000000\n      1462.000000\n      1462.000000\n      1462.000000\n      1462.000000\n    \n    \n      mean\n      3129.981826\n      5453.087743\n      47.672367\n      4.096717\n      18307.376881\n    \n    \n      std\n      6950.548042\n      10884.467444\n      21.613905\n      0.289497\n      42766.096572\n    \n    \n      min\n      39.000000\n      39.000000\n      0.000000\n      2.000000\n      2.000000\n    \n    \n      25%\n      325.000000\n      800.000000\n      32.000000\n      4.000000\n      1191.500000\n    \n    \n      50%\n      799.000000\n      1670.000000\n      50.000000\n      4.100000\n      5179.000000\n    \n    \n      75%\n      1999.000000\n      4321.250000\n      63.000000\n      4.300000\n      17342.250000\n    \n    \n      max\n      77990.000000\n      139900.000000\n      94.000000\n      5.000000\n      426973.000000\n    \n  \n\n\n\n\n\ndf.describe(include = 'all')\n\n\n\n\n\n  \n    \n      \n      product_id\n      product_name\n      category\n      discounted_price\n      actual_price\n      discount_percentage\n      rating\n      rating_count\n      about_product\n      user_id\n      user_name\n      review_id\n      review_title\n      review_content\n      img_link\n      product_link\n    \n  \n  \n    \n      count\n      1462\n      1462\n      1462\n      1462.000000\n      1462.000000\n      1462.000000\n      1462.000000\n      1462.000000\n      1462\n      1462\n      1462\n      1462\n      1462\n      1462\n      1462\n      1462\n    \n    \n      unique\n      1348\n      1334\n      211\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1290\n      1191\n      1191\n      1191\n      1191\n      1209\n      1409\n      1462\n    \n    \n      top\n      B07JW9H4J1\n      Fire-Boltt Ninja Call Pro Plus 1.83\" Smart Wat...\n      Computers&Accessories|Accessories&Peripherals|...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      [CHARGE & SYNC FUNCTION]- This cable comes wit...\n      AHIKJUDTVJ4T6DV6IUGFYZ5LXMPA,AE55KTFVNXYFD5FPY...\n      $@|\\|TO$|-|,Sethu madhav,Akash Thakur,Burger P...\n      R3F4T5TRYPTMIG,R3DQIEC603E7AY,R1O4Z15FD40PV5,R...\n      Worked on iPhone 7 and didn‚Äôt work on XR,Good ...\n      I am not big on camera usage, personally. I wa...\n      https://m.media-amazon.com/images/I/413sCRKobN...\n      https://www.amazon.in/Wayona-Braided-WN3LG1-Sy...\n    \n    \n      freq\n      3\n      5\n      231\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      6\n      10\n      10\n      10\n      10\n      8\n      3\n      1\n    \n    \n      mean\n      NaN\n      NaN\n      NaN\n      3129.981826\n      5453.087743\n      47.672367\n      4.096717\n      18307.376881\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      std\n      NaN\n      NaN\n      NaN\n      6950.548042\n      10884.467444\n      21.613905\n      0.289497\n      42766.096572\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      min\n      NaN\n      NaN\n      NaN\n      39.000000\n      39.000000\n      0.000000\n      2.000000\n      2.000000\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      25%\n      NaN\n      NaN\n      NaN\n      325.000000\n      800.000000\n      32.000000\n      4.000000\n      1191.500000\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      50%\n      NaN\n      NaN\n      NaN\n      799.000000\n      1670.000000\n      50.000000\n      4.100000\n      5179.000000\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      75%\n      NaN\n      NaN\n      NaN\n      1999.000000\n      4321.250000\n      63.000000\n      4.300000\n      17342.250000\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      max\n      NaN\n      NaN\n      NaN\n      77990.000000\n      139900.000000\n      94.000000\n      5.000000\n      426973.000000\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN"
  },
  {
    "objectID": "posts/Data-Analysis-of-Amazon-Sales-Dataset.html#price-rating",
    "href": "posts/Data-Analysis-of-Amazon-Sales-Dataset.html#price-rating",
    "title": "Sentiment Analysis in NLP of Amazon Sales Data Project",
    "section": "Price & Rating",
    "text": "Price & Rating\n\na. What is the relationship between discount price and user rating?\n\n# Cleaning Product Category\ndf['grouped_category'] = df['category'].str.split('|').str[0]\ndf.drop('category', axis=1, inplace=True)\n\n\ndata1 = {'count':df.groupby(['grouped_category'])['grouped_category'].count(),\n         'mean_discounted_price': df.groupby(['grouped_category'])['discounted_price'].mean(),\n         'mean_discount_percentage': df.groupby(['grouped_category'])['discount_percentage'].mean(),\n         'mean_rating': df.groupby(['grouped_category'])['rating'].mean()}\n\ndata1 = pd.DataFrame(data1)\ndata1 = data1.reset_index()\n\n\nimport plotly_express as px\n\n\npx.scatter(data1,\n           x=\"mean_rating\", \n           y=\"mean_discount_percentage\",\n           hover_name='grouped_category', color = 'grouped_category', \n           size = 'count',size_max = 80,\n           labels={\n                     \"mean_discount_percentage\": \"Mean Discount Percentage (%)\",\n                     \"mean_rating\": \"Mean Rating\",\n                     \"grouped_category\": \"Product Category\"\n                 },\n           title = \"Would Users' Rating Being Influenced by the Discount Level of the Product?\"\n          )\n\n\n                                                \n\n\n\n\nAnalytical Process of Visualizations 1.1\nTo better analyze the various data within each product category, I first created a new dataset, grouped by different product categories, containing average discount prices, average discount percentages, and average ratings. The visualization above is the scatter plot of the relationship between the mean rating of products of each product category and their mean discount percentage. The size is represented by the total number of the products in correspounding category; the color is based on different product categories. Since there are some categories have only less than 10 products, which is hard to be seen in the graph, I manually adjust the size_max to make them more easier to see.\n\n\nFinding\nFirst, from the visualization, we can clearly see that categories ‚ÄúHome&Kitchen,‚Äù ‚ÄúElectronics,‚Äù and ‚ÄúComputers&Accesories‚Äù are the top three categories that have the most products belong to them; where category ‚ÄúElectronics‚Äù has the most products in this dataset. Except for the ‚ÄúOffice Products‚Äù and ‚ÄúToys & Games‚Äù categories in the lower right corner, we can see that the remaining product categories basically follow the same pattern of the higher the rating, the higher the percentage of discounts. Similarly, when we focus on just the three product categories with the largest representation (as the others are less representative), we notice essentially the same pattern. Although the rating gap in the data is not very significant, we in turn would be able to conclude that the level of product discount has a positive impact on the user‚Äôs rating of the product to some extent, i.e.¬†the higher the discount, the higher the rating."
  },
  {
    "objectID": "posts/Data-Analysis-of-Amazon-Sales-Dataset.html#natural-language-processing-nlp",
    "href": "posts/Data-Analysis-of-Amazon-Sales-Dataset.html#natural-language-processing-nlp",
    "title": "Sentiment Analysis in NLP of Amazon Sales Data Project",
    "section": "Natural Language Processing (NLP)",
    "text": "Natural Language Processing (NLP)\n\na. Which words are most commonly used by users in their review title and content?\n\n1. Tokenization\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\na = list(df['review_title'])\nreview_title = ' '.join(str(b) for b in a)\n\nc = list(df['review_content'])\nreview_content = ' '.join(str(e) for e in c)\n\n\ntitle = []\nfor w in word_tokenize(review_title.lower()):\n    title.append(w)\n\ncontent = []\nfor i in word_tokenize(review_content.lower()):\n    content.append(i)\n\n\n\n2. Remove Stopwords\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\nbad_chars = [\"'s\", \"--\",\"n't\", \"..\"]\nmyStopWords = list(punctuation) + stopwords.words('english') + list(bad_chars)\n\n\nwordsNoStop = [w for w in title if w not in myStopWords]\nwordsNoStop_content = [i for i in content if i not in myStopWords]\n\n\n\n3. Stemming and Tagging\n\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.stem.porter import PorterStemmer\n\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n\n\n\nwordPorterStems = [PorterStemmer().stem(w) for w in wordsNoStop]\nwordPorterStems_content = [PorterStemmer().stem(i) for i in wordsNoStop_content]\n\n\npos_tag_title = nltk.pos_tag(wordsNoStop)\npos_tag_content = nltk.pos_tag(wordsNoStop_content)\n\n\n\n4. Word frequencies\n\nfrom nltk.probability import FreqDist\n\n\nfreq_title = FreqDist(wordsNoStop)\n\n\nfreq_content = FreqDist(wordsNoStop_content)\n\n\n# Get the most common top 15 words in review title and content\n\n\ndf_title = pd.DataFrame({'title': FreqDist(wordsNoStop).keys(),\n                        'title_freq': FreqDist(wordsNoStop).values()})\ndf_title = df_title.sort_values(by = 'title_freq', ascending = False).reset_index()\n\n\n\n\n\n  \n    \n      \n      index\n      title\n      title_freq\n    \n  \n  \n    \n      0\n      8\n      good\n      3854\n    \n    \n      1\n      6\n      product\n      2289\n    \n    \n      2\n      27\n      nice\n      882\n    \n    \n      3\n      9\n      quality\n      732\n    \n    \n      4\n      5\n      money\n      539\n    \n  \n\n\n\n\n\nprint('Standard Deviation of Review Title: ', df_title['title_freq'].std())\n\nStandard Deviation of Review Title:  74.27967808423287\n\n\n\ndf_content = pd.DataFrame({\n                        'content': FreqDist(wordsNoStop_content).keys(),\n                        'content_freq': FreqDist(wordsNoStop_content).values()})\ndf_content = df_content.sort_values(by = 'content_freq', ascending = False).reset_index()\n\n\n\n\n\n  \n    \n      \n      index\n      content\n      content_freq\n    \n  \n  \n    \n      0\n      8\n      good\n      6043\n    \n    \n      1\n      13\n      product\n      3640\n    \n    \n      2\n      19\n      quality\n      2278\n    \n    \n      3\n      382\n      use\n      1604\n    \n    \n      4\n      67\n      one\n      1359\n    \n  \n\n\n\n\n\nprint('Standard Deviation of Review Content: ', df_content['content_freq'].std())\n\nStandard Deviation of Review Content:  69.32795679884457\n\n\n\n\n5. Visualization of word frequencies\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n\nfig = make_subplots(rows = 1, cols = 2,\n                    subplot_titles = (\"Word Frequency of Review Title\",\n                                    \"Word Frequency of Review Content\"))\n\nfig.add_trace(\n    go.Scatter(\n        x = df_title['title_freq'].head(n = 15),\n        y = df_title['title'].head(n = 15),\n        name = 'Review Title'\n    ),\n    row = 1, col = 1\n)\n\nfig.add_trace(\n    go.Scatter(\n        x = df_content['content_freq'].head(n = 15),\n        y = df_content['content'].head(n = 15),\n        name = 'Review Content'\n    ),\n    row = 1, col = 2\n)\nfig.update_xaxes(title_text = \"Word Count\", row = 1, col = 1)\nfig.update_xaxes(title_text = \"Word Count\", row = 1, col = 2)\n\nfig.update_yaxes(title_text=\"Word\", row = 1, col = 1)\n\nfig.update_layout(height = 600, width = 800)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\nAnalytical Process of Visualizations 2.1\nFirst I conducted natural language processing: tokenization, stopwords removal, stemming, and tagging for all review titles and content in the dataset in order to obtain a clean text for all titles and content. Then I created two new datasets based on the frequency of words in the review titles and content, including each individual word and its corresponding total frequency of occurrence. I then further exam their standard deviation. Finally, I visualized the top 15 words in terms of frequency.\n\n\nFinding\nIn the review title plot, we can get a rough idea of the user‚Äôs attitude towards the product, i.e.¬†which aspects of performance/attributes are important to the user. The most frequent words we see are ‚Äògood‚Äô, followed by ‚Äòproduct,‚Äô ‚Äònice,‚Äô ‚Äô quality,‚Äô ‚Äòmoney,‚Äô ‚Äòbest,‚Äô and ‚Äòvalue.‚Äô Based on these words, we can in fact get a approximately understand that the customer reviews of the products in this dataset are generally positive; and that the customers pay more attention to the quality and price of the products and the value between them.\nIn the content of the reviews we can expect the same conclusions, only with more description of the product. Besides the first few words ‚Äògood,‚Äô ‚Äòproduct,‚Äô and ‚Äòquality‚Äô which are virtually the same as in the titles of the reviews we see the words ‚Äòuse,‚Äô ‚Äòcable,‚Äô and ‚Äòphone,‚Äô which allows us to learn that the majority of products and user purchases in this dataset are electronically related, in line with the the popular product categories we found earlier.\nFinally, looking at the number of standard deviations for review titles and review contents, which are around 74.3 and 69.3, respectively, they are both very high. Since the standard deviation is a measure of the dispersion of the data in terms of the mean, we can say that the word frequencies of the review titles and contents are very dispersed."
  },
  {
    "objectID": "posts/Data-Analysis-of-Amazon-Sales-Dataset.html#sentiment-analysis",
    "href": "posts/Data-Analysis-of-Amazon-Sales-Dataset.html#sentiment-analysis",
    "title": "Sentiment Analysis in NLP of Amazon Sales Data Project",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\na. What is the overall sentiment of users review in the dataset?\n\nnltk.download('vader_lexicon')\nfrom nltk.sentiment import vader\nsia = vader.SentimentIntensityAnalyzer()\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\n\n# define a function that get review content for every rows in dataset\ndef getSentiment(i):\n    return sia.polarity_scores(i)['compound']\n\n\ndf_new = df[['rating', 'review_content', 'grouped_category']]\ndf_new['sentiment'] = [getSentiment(content) for content in df_new['review_content']]\n\n/tmp/ipykernel_313/1918612639.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\nelec = df_new.loc[(df_new['grouped_category'] == 'Electronics')]\ncomputer = df_new.loc[(df_new['grouped_category'] == 'Computers&Accessories')]\nhome = df_new.loc[(df_new['grouped_category'] == 'Home&Kitchen')]\noffice = df_new.loc[(df_new['grouped_category'] == 'OfficeProducts')]\n\n\ntrace1 = go.Histogram(\n                x = elec.sentiment,\n                name = \"Electronics\", nbinsx=25)\ntrace2 = go.Histogram(\n                 x = home.sentiment,\n                name = \"Home & Kitchen\")\ntrace3 = go.Histogram(\n                x = computer.sentiment,\n                name = \"Computers&Accessories\")\ntrace4 = go.Histogram(\n                x = office.sentiment,\n                name = \"OfficeProducts\")\n\ndata = [trace1, trace2, trace3, trace4]\n\nlayout = go.Layout(barmode = \"group\")\n\nfig = go.Figure(data = data, layout = layout)\n\nfig.update_layout(xaxis=dict(range=[0,1]), title = 'Overall Sentiment of Review Content')\nfig.update_xaxes(title_text = \"Sentiment Compound\")\n\nfig.update_yaxes(title_text=\"Count\")\n\nfig.show()\n\n\n                                                \n\n\n\n\nAnalytical Process of Visualizations 3.1\nFor the sentiment analysis, I mainly used SentimetnIntensityAnalyzer from the vader analysis package. After defining a function that would get the sentiment of each product review, I then created a new dataset with product reviews, ratings, categories and sentiment compound scores. Further I created sub-datasets of the top four product categories based on their popularity for visualization purposes. Subsequently I created visualizations based on the counts of their sentiment compound scores. Only the sentiment scores from 0-1.0 are shown in the graph, but the visualization of the negative sentiment scores can be obtained by dragging the horizontal axis with the mouse.\n\n\nFinding\nSure enough, as we predicted earlier, most products fall into the range of 0.9-1.0 sentiment scores. This not only proves that customer satisfaction with the products is generally high, but also further demonstrates the limitations of this dataset. It is possible that due to the overall small number of products, the results are not very differentiated and may also be relatively unrepresentative. However, in general, the overall customer review sentiment in this dataset is relatively very positive.\n\n\nb. Which words are used most often in negative/positive reviewsÔºü\n\nfrom wordcloud import WordCloud, ImageColorGenerator\n\n\npositive =' '.join([i for i in df_new['review_content'][df_new['sentiment'] > 0.5]])\nwordcloud = WordCloud(stopwords=myStopWords, background_color=\"black\", \n                      collocations=False, colormap = 'Set3').generate(positive)\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Word Cloud of Positive Customer Reviews')\nplt.show()\n\n\n\n\n\nnegative =' '.join([i for i in df_new['review_content'][df_new['sentiment'] < -0.5]])\nwordcloud = WordCloud(stopwords=myStopWords, background_color=\"black\", \n                      collocations=False, colormap = 'Set3').generate(negative)\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\n\nplt.title('Word Cloud of Negative Customer Reviews')\nplt.show()\n\n\n\n\n\n\nAnalytical Process of Visualizations 3.2\nBased on the analysis in the previous section, I went a step further and generated word clouds in the negative and positive review content based on the sentiment scores. Since most of the comments were classified as positive, here I used 0.5 and -0.5 as the dividing line between negative and positive to better show the difference/similarity between them.\n\n\nFinding\nBased on the positive word cloud content, as in the previous review content word frequency distribution plot, we saw many positive words. Not only are customers satisfied with the quality of the product itself, we also capture words such as ‚Äòtime‚Äô and ‚ÄòAmazon,‚Äô which could be a side indication that customers are reacting positively to values such as the quality of the product, and we can speculate that they are also satisfied with Amazon as platform itself, as well as the delivery time.\nIn the negative rating content we unusually observed a high frequency of the word ‚Äúgood‚Äù. Although most of the words are related to positive content, we can infer that they occur in a different context than the previous word cloud based on the negative sentiment scores. This means that although vader marks some comments as negative, remembering that most of the comments in this dataset are very positive, there is an inevitable problem of under-representation in presenting negative word clouds. As a whole, we can deduce that in negative reviews, customers are likely to set the overall tone of the review as ‚Äògood,‚Äô i.e.¬†they would write comments like ‚Äúthis product is good, but xxx,‚Äù and the word ‚Äòbut‚Äô is usually more appealing to machine sentiment recognition systems. That is, in addition to the obvious comment about a product being bad/poor, a customer may feel good about the quality of the product in general, but be dissatisfied with some specific aspects, such as ‚Äúservice,‚Äù or ‚Äúinstallation,‚Äù and therefore be rated as a negative comment.\n\n\nc.¬†Is there a correlation between sentiments and ratings?\n\npx.scatter(df_new,\n           x='sentiment',\n           y='rating',\n           color = 'grouped_category',\n           hover_name='grouped_category', \n           title = \"Do more positive reviews lead to higher ratings? Or vice versa?\")\n\n\n                                                \n\n\n\n# Calculate the P-value between variable sentiment and rating\nnp.corrcoef(df_new['sentiment'], df_new['rating'])\n\narray([[1.        , 0.23779857],\n       [0.23779857, 1.        ]])\n\n\n\n\nAnalytical Process of Visualizations 4.1\nIn the last section, I made a visualization based on sentiment and rating scales to represent the relationship between them. I mainly wanted to know if there is a positive correlation between the customers‚Äô ratings and their review sentiment. In addition, I calculated the correlation coefficient using Numpy‚Äôs corrcoef() function, which allowed me to better understand the exact relationship between the two variables. This function will return a matrix of correlation coefficients, where correlation[0][1] and [1][1] represent the correlation between sentiment and reviews.\n\n\nFinding\nFirst of all, from the plot, it‚Äôs clear that this doesn‚Äôt have a clue, with most of the data scattered between ratings of 3.0-4.5, and sentiment doesn‚Äôt seem to make a difference to the ratings, especially between sentiment scores of 0.5-1.0, with ratings unevenly distributed from 2.5 all the way up to 5.0. And looking at the correlation coefficient, we got a value of about 0.2. This number is very close to 0, which means that it does not represent any correlation. As a result from here we can confidently conclude that the sentiment scores and customer ratings in this dataset do not have a significant correlation.\nThis turned out to be different than what I thought when I first obtained the dataset, but then as I explored this dataset in depth, I did have hypotheses about how we might not get representative and meaningful results in terms of the relationship between the review sentiment scores and the ratings. There are two reasons for this, firstly, most of our sentiment scores lean towards a very positive range and the ratings are also concentrated between 3.4-4.5 which in effect causes these two sets of data spread evenly across these ranges and therefore it is likely that we will not get a significant correlation; secondly, as mentioned earlier, the overall amount of data in this dataset is small and there is an ontological bias.\nNevertheless, we could still get some slightly meaningful information from the plot. For instance, in the Computers&Accessories category, its product rating stays above 3.25 regardless of the customer‚Äôs review sentiment score, which also applies to the Office Products category, while the two remaining product categories with a large representation ‚ÄúHome & Kitchen‚Äù and ‚ÄúElectroncs‚Äù have a much wider (and lower) range of ratings."
  },
  {
    "objectID": "posts/first_file.html",
    "href": "posts/first_file.html",
    "title": "First File",
    "section": "",
    "text": "## \n\n\n3 + 3\n\n6\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.plot([1,5,3], \"ko\")"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "lisasblog",
    "section": "",
    "text": "Sentiment Analysis in NLP of Amazon Sales Data Project\n\n\n\n\n\nFinal Project for DH140\n\n\n\n\n\n\nMar 6, 2023\n\n\nShiqi Ouyang\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst File\n\n\n\n\n\nTesting Quarto\n\n\n\n\n\n\nMar 6, 2023\n\n\nLisa Ouyang\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\nNo matching items"
  }
]